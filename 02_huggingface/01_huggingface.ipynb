{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Hugging Face\n",
    "---\n",
    "## 01_01 Hugging Face란?\n",
    "* 코드로 구현되어 있는 모델을 모아놓은 곳으로 AI의 github라 불린다.\n",
    "* 허깅페이스는 최신 AI 기술을 더 간편하게 활용 할 수 있또록 제공되는 플랫폼이며, 핵심기술은 Transformers 라이브러리이다.\n",
    "\n",
    "### 01_01_01 Hugging Face 장점\n",
    "* 모델을 처음부터 개발하고 훈련할 필요가 없다.\n",
    "* 사용 편의성 (간단하고 사용자 친화적인 인터페이스가 제공되어 쉽게 이용가능)\n",
    "* 지속적으로 발전 중 (AI의 최신기술을 공유받고 쉽게 접근 가능)\n",
    "\n",
    "\n",
    "## 01_01 Transformers 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_01 자연어처리\n",
    "* 컴퓨터가 인간의 언어를 이해, 생성, 조작 할 수 있도록 해주는 인공지능의 한 분야\n",
    "\n",
    "**자연어 처리의 주요 단계**\n",
    "\n",
    "* 토큰화: 텍스트를 의미 있는 단위(토큰)로 나누는 과정  \n",
    "    예시: \"나는 학교에 간다\" -> [\"나는\", \"학교에\", \"간다\"]<br>\n",
    "\n",
    "* 단어 집합 생성: 텍스트에서 고유한 단어들을 모아 사전을 만드는 과정  \n",
    "    예시: [\"나는\", \"학교에\", \"간다\"] -> {\"나는\", \"학교에\", \"간다\"}<br>\n",
    "\n",
    "* 정수 인코딩: 단어를 고유한 정수로 변환하여 컴퓨터가 처리할 수 있게 하는 과정  \n",
    "    예시: {\"나는\": 1, \"학교에\": 2, \"간다\": 3} -> [1, 2, 3]<br>\n",
    "\n",
    "* 패딩: 서로 다른 길이의 시퀀스를 동일한 길이로 맞추는 과정  \n",
    "    예시: [1, 2, 3] -> [1, 2, 3, 0, 0] (길이 5로 패딩)<br>\n",
    "\n",
    "* 벡터화: 텍스트 데이터를 숫자 벡터로 변환하여 기계학습 모델에 입력할 수 있게 하는 과정  \n",
    "    예시: \"나는\" -> [0.1, 0.2, 0.3, ..., 0.9] (100차원 벡터)  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**형태소 토큰화**  \n",
    "<br>\n",
    "원시문 예시 :  \n",
    "\"길을 가다가 고양이를 봤어. 고양이에게 츄르를 주려했는데 고양이가 도망가버렸어\"  \n",
    "\n",
    "토큰화 예시 :  \n",
    "[\"길을\", \"가다가\", \"고양이를\", \"봤어\", \".\", \"고양이에게\", \"츄르를\", \"주려했는데\", \"고양이가\", \"도망가버렸어\"]  \n",
    "\n",
    "똑같은 고양이가 총 3번 등장한다, 이때 \"를\", \"에게\", \"가\" 가 붙어있어 기계는 각기 다른 단어로 인식 할 수 있다.  \n",
    "따라서 한국에서는 보편적으로 '형태소 분석기'로 토큰화를 진행한다.  \n",
    "\n",
    "형태소 토큰화 예시 : \n",
    "[\"길\", \"을\", \"가다가\", \"고양이\", \"를\", \"보\", \"았\", \"어\", \".\", \"고양이\", \"에게\", \"츄르\", \"를\", \"주\", \"려\", \"하\", \"았\", \"는데\", \"고양이\", \"가\", \"도망가\", \"버리\", \"었\", \"어\"]  \n",
    "\n",
    "이렇게 형태소 단위로 분리하면 \"고양이\"라는 단어가 일관되게 인식되며, 조사나 어미는 별도로 분리된다.  \n",
    "이를 통해 기계가 더 정확하게 단어의 의미와 문법적 기능을 파악할 수 있다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 발전 과정\n",
    "\n",
    "**카운트 기반 단어 표현**\n",
    "* 단어 표현 중 하나로, 단어의 빈도수를 기반으로 단어의 의미를 표현하는 방법\n",
    "* DTM(Document Term Matrix) : 문서 내 단어의 빈도수를 기반으로 문서를 표현하는 방법\n",
    "* TF-IDF : 단어의 중요도를 고려하여 문서를 표현하는 방법 <br>\n",
    "\n",
    "Bag of Words(단어주머니)\n",
    "* 문서 내 단어의 출현 여부를 기반으로 문서를 표현하는 방법\n",
    "* 단어의 출현 빈도로만 나타내기 때문에 단어의 순서는 고려하지 않는다.\n",
    "<br> => 문맥 파악이 불가능 (강아지가 놀고 있다 와 놀고 있는 강아지를 구별 못함)\n",
    "\n",
    "**단어임베딩(통계기반의 언어모델)**\n",
    "* 대표적인 알고리즘 : Word2vec, GloVe\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* 단어의 의미를 벡터로 표현하여 문맥을 고려할 수 있도록 해준다.\n",
    "<br> => 하지만 다의어 같은 (문맥에따라 다양한의미를 기지는걸 구별하지 못함)\n",
    "\n",
    "**딥러닝 기반 언어 모델**\n",
    "\n",
    "**시퀀스 모델**\n",
    "* 시퀀스란?\n",
    "<br>시퀀스는 순서가 있는 데이터의 집합을 의미한다.\n",
    "<br>자연어 처리에서 시퀀스는 주로 단어나 문자의 연속을 나타낸다.\n",
    "<br>예를 들어, 문장은 단어들의 시퀀스이고, 단어는 문자들의 시퀀스입니다.\n",
    "<br>시퀀스 데이터는 순서가 중요하며, 이전 요소가 다음 요소에 영향을 미칠 수 있다.<br>\n",
    "\n",
    "* RNN(Recurrent Neural Network)<br> \n",
    "순차적 데이터를 처리하는 신경망으로, 이전 단계의 정보를 현재 단계로 전달하는 구조를 가짐\n",
    "* LSTM(Long Short-Term Memory)<br>\n",
    "RNN의 장기 의존성 문제를 해결하기 위해 개발된 모델로, 장기 기억을 유지할 수 있는 구조를 가짐\n",
    "* GRU(Gated Recurrent Unit)<br>\n",
    "LSTM을 간소화한 모델로, 더 적은 매개변수를 사용하면서도 비슷한 성능을 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 Transformers 라이브러리란?\n",
    "* 허깅페이스에서 제공하는 최신 모델을 쉽게 사용할 수 있도록 도와주는 라이브러리이다.\n",
    "* 트랜스포머는 자연어 처리 분야에서 많이 사용되는 모델로, 다양한 언어 모델을 포함하고 있다.\n",
    "\n",
    "트랜스포머는 자연어 처리 분야의 주요 딥러닝 모델로, \"Attention is All You Need\" 논문에서 소개되었다.  \n",
    "Self-Attention 메커니즘을 사용하여 문맥 이해와 단어 간 관계 파악에 효과적이다.  \n",
    "기계 번역, 텍스트 생성, 질문 응답 등 다양한 작업에서 우수한 성능을 보인다.\n",
    "\n",
    "쉽게 생각하면 컴퓨터가 말하는 방법을 배우는 것과 비슷한 것으로 문장 속의 단어들이 어떤 관계를 가지고 있는지 파악하고 문장의 뜻을 이해하는데 기술이다\n",
    "\n",
    "* Transformers를 통해 사전학습된 최신 모델을 쉽게 다운로드하고 학습할 수 있는 API와 도구를 제공한다.\n",
    "\n",
    "**지원하는 기능**\n",
    "1) 자연어 처리 : 텍스트 분류, 엔티티 인식, 질문답변, 언어모델링, 요약, 번역, 객관식 텍스트 생성\n",
    "2) 컴퓨터 비전 : 이미지 분류, 물체감지 및 분할\n",
    "3) 오디오 : 음성인식 및 오디오 분류\n",
    "4) 멀티모달 : 테이블 질문 답변, 광학문자 인식, 스캔문서에서 정보추출, 비디오분류, 시각적 질문답변\n",
    "\n",
    "멀티모달이란 : 텍스트 이미지, 음성, 영상 등 다양한 양식으로 훈련해 다양한 결과물을 낼 수 있는 모델\n",
    "-> 즉, 여러가지 유형의 데이터를 동시에 다루는 기술\n",
    "\n",
    "https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이토치<br>\n",
    "https://pytorch.kr/get-started/locally/\n",
    "<br>\n",
    "- 트랜스포머스<br>\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5056, 0.5386, 0.6537],\n",
      "        [0.8546, 0.4736, 0.2444],\n",
      "        [0.4393, 0.3350, 0.5750],\n",
      "        [0.2359, 0.0572, 0.9102],\n",
      "        [0.3060, 0.7150, 0.7555]])\n",
      "4.45.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 사용방법\n",
    "\n",
    "**pipeline() 함수**\n",
    "* 다양한 task를 위한 인터페이스를 제공하여, 복잡한 모델을 쉽게 사용 할 수 있게 해준다.\n",
    "* 특정 모델과 동작에 필요한 전처리 및 후처리 단계를 연결하여 텍스트를 직접 입력하고 이해하기 쉬운 답변을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline()함수의 인자**\n",
    "\n",
    "1. 공통인자\n",
    "* task : 수행할 작업의 종류를 지정\n",
    "* model : 사용할 모델의 이름이나 경로를 지정\n",
    "* tokenizer : 사용할 토크나이저 지정 (모델과 함께 제공되는 토크나이저를 사용하는게 일반적이다.)\n",
    "* framework : 사용할 딥러닝 프레임워크 지정 (pt : 파이토치, tf : 텐서플로우)\n",
    "* device : 실행할 장치 (CPU=> -1, GPU=>0), cuda\n",
    "\n",
    "2. 작업별 인자\n",
    "* text-generation:\n",
    "    * max_length : 생성할 텍스트의 최대길이 지정\n",
    "    * num_return_sequence : 생성할 텍스트의 개수 지정\n",
    "    * temperature : 무작위성을 조절하는 파라미터\n",
    "<br>\n",
    "* question-answering :\n",
    "    * context : 질문에 대한 답을 찾을 문맥 제공\n",
    "    * question : 답을 찾고자 하는 질문\n",
    "<br>\n",
    "* translation:\n",
    "    * src_lang : 원본 텍스트의 언어 지정\n",
    "    * tgt_lang : 번역할 목표 언어 지정\n",
    "<br>\n",
    "* summarization:\n",
    "    * min_length : 요약문에서 생성할 최소 단어 수 지정\n",
    "    * max_length : 요약문에서 생성할 최대 단어 수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# transformers 패키지에서 pipeline을 import\n",
    "from transformers import pipeline\n",
    "\n",
    "# 음성을 인식해서 텍스트로 출력할 수 있는 모델\n",
    "# STT (automatic-speech-recognition) : 음성인식\n",
    "transcriber = pipeline('automatic-speech-recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마틴루터킹 목사 연설 중 일부\n",
    "# I have a dream that one day this natio will rise up and live out the true meaning of its creed\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51b7af08f2b4fdda0aed5d577d4cc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\80416\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\80416\\.cache\\huggingface\\hub\\models--openai--whisper-large-v3-turbo. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5817348690ba4eb4adeb969ce1994075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ce874e72314d6aa37f0c8aaff8de0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3195e7b27124208a195f6fc97a71b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de94906bb09461ab8c354a3963fec4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a1eb5a2a77447387aff5f4a9ea68d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371b42fd0e8f4918a7b81ea29e2c65c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba6fa26ad324ee6aee37d808dfcdcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41001e825ff9484da0c6fa35d986941c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84d979d6de546b7888015eb5708d339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b692f62d19204e5990ea8348cdfb91f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\80416\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v3-turbo\")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoClass\n",
    "* 사전학습된 모델을 쉽게 사용할 수 있도록 도와주는 클래스\n",
    "* 수행하고자하는 task에 적합한 AutoModel과 AutoTekenizer를 선택해준다.\n",
    "\n",
    "| 구성요소 | 목적 | 특성 | 데이터 타입 | 변형 과정 | 결과물 | 주요 사용 사례 |\n",
    "|---------|------|------|------------|-----------|--------|---------------|\n",
    "| 사전 훈련된 토크나이저 | 텍스트 데이터를 모델이 처리할 수 있는 형태로 변환 | 텍스트 토큰화, 인코딩, 어휘사전 관리 | 텍스트 | 토큰화, 인코딩 | 모델이 처리할 수 있는 텍스트 형태 | 자연어 이해 및 생성 작업 |\n",
    "| 사전 훈련된 이미지 프로세서 | 이미지 데이터를 모델이 처리할 수 있는 형태로 변환 | 이미지 크기 조정, 정규화, 텐서 변환 | 이미지 | 크기 조정, 정규화 | 모델이 처리할 수 있는 이미지 형태 | 이미지 분류, 객체 감지 |\n",
    "| 사전 훈련된 특성 추출기 | 데이터로부터 중요한 특성을 추출 | 다양한 데이터 타입 처리, 중요 특성 식별 | 다양한 데이터 | 중요 특성 추출 | 분석 및 모델 입력에 필요한 특성 | 데이터 분석, 모델 학습 |\n",
    "| 사전 훈련된 프로세서 | 다양한 데이터 타입의 전처리를 통합 관리 | 텍스트, 이미지 등 다양한 데이터 처리 | 텍스트, 이미지 등 | 통합 전처리 | 다양한 형태의 데이터 처리 | 멀티모달 입력을 가진 작업 |\n",
    "| 사전 훈련된 모델 | 특정 작업을 수행하는 데 바로 사용 가능 | 다양한 아키텍처, 사전 훈련된 가중치 | 텍스트, 이미지 | 가중치 튜닝 | 작업 수행(분류, 생성 등) | 자연어 및 이미지 처리 작업 |\n",
    "| AutoBackbone | 이미지 데이터를 효율적으로 처리할 수 있도록 모델 아키텍처 제공 | 사전 훈련된 가중치 사용, 다양한 모델 지원 가능성 제공 | 이미지 | 아키텍처 선택 및 튜닝 | 이미지 관련 작업에 적합한 모델 구조 | 이미지 분류, 객체 인식, 특성 추출 등 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AutoTokenizer\n",
    "* 텍스트를 모델이 처리할 수 있는 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112723cd1dfb4888bd72ce5a2860d688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\80416\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\80416\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f200ab918704a54ac59a7eacad7db92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca68bdf37e1148c48e9c66bb3eb2617d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0115061e224ed58513407680d944e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토큰화만 진행\n",
    "# google-bert/bert-base-uncased 모델에 최적화된 토크나이저를 찾아내고 다운로드하여 사용할 수 있게 해준다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Hello, my dog is cute\"\n",
    "print(tokenizer(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. AutoImageProcessor\n",
    "* 이미지 데이터를 모델이 처리할 수 있는 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[ 0.1137,  0.1686,  0.1843,  ..., -0.1922, -0.1843, -0.1843],\n",
      "          [ 0.1373,  0.1686,  0.1843,  ..., -0.1922, -0.1922, -0.2078],\n",
      "          [ 0.1137,  0.1529,  0.1608,  ..., -0.2314, -0.2235, -0.2157],\n",
      "          ...,\n",
      "          [ 0.8353,  0.7882,  0.7333,  ...,  0.7020,  0.6471,  0.6157],\n",
      "          [ 0.8275,  0.7961,  0.7725,  ...,  0.5843,  0.4667,  0.3961],\n",
      "          [ 0.8196,  0.7569,  0.7569,  ...,  0.0745, -0.0510, -0.1922]],\n",
      "\n",
      "         [[-0.8039, -0.8118, -0.8118,  ..., -0.8902, -0.8902, -0.8980],\n",
      "          [-0.7882, -0.7882, -0.7882,  ..., -0.8745, -0.8745, -0.8824],\n",
      "          [-0.8118, -0.8039, -0.7882,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.2706, -0.3176, -0.3647,  ..., -0.4275, -0.4588, -0.4824],\n",
      "          [-0.2706, -0.2941, -0.3412,  ..., -0.4824, -0.5451, -0.5765],\n",
      "          [-0.2784, -0.3412, -0.3490,  ..., -0.7333, -0.7804, -0.8353]],\n",
      "\n",
      "         [[-0.5451, -0.4667, -0.4824,  ..., -0.7412, -0.6941, -0.7176],\n",
      "          [-0.5529, -0.5137, -0.4902,  ..., -0.7412, -0.7098, -0.7412],\n",
      "          [-0.5216, -0.4824, -0.4667,  ..., -0.7490, -0.7490, -0.7647],\n",
      "          ...,\n",
      "          [ 0.5686,  0.5529,  0.4510,  ...,  0.4431,  0.3882,  0.3255],\n",
      "          [ 0.5451,  0.4902,  0.5137,  ...,  0.3020,  0.2078,  0.1294],\n",
      "          [ 0.5686,  0.5608,  0.5137,  ..., -0.2000, -0.4275, -0.5294]]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# google/vit-base-patch16-224 모델을 사용하는데 최적화된 이미지 프로세서(전처리기)를 가져온다.\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# 이미지 다운로드\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# 이미지 전처리\n",
    "inputs = image_processor(images=image, return_tensors='pt')\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 추론 결과 출력\n",
    "logits = outputs.logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
